{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76495bf0-f6e3-40dd-818f-9b025bc7cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE SPARK MBA NOTEBOOK ===\n",
      "Dataset: online_retail_II.xlsx (2 sheets)\n",
      "Expected records: ~1.6 million\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import all libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== COMPLETE SPARK MBA NOTEBOOK ===\")\n",
    "print(\"Dataset: online_retail_II.xlsx (2 sheets)\")\n",
    "print(\"Expected records: ~1.6 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4c40e1-bb3d-4217-9fb8-b5277331cfc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "aiohappyeyeballs        2.6.1\n",
      "aiohttp                 3.12.15\n",
      "aiosignal               1.4.0\n",
      "asttokens               3.0.0\n",
      "attrs                   25.3.0\n",
      "certifi                 2025.8.3\n",
      "charset-normalizer      3.4.3\n",
      "click                   8.2.1\n",
      "cloudpickle             3.1.1\n",
      "comm                    0.2.3\n",
      "complete                0.0.1\n",
      "contourpy               1.3.3\n",
      "cycler                  0.12.1\n",
      "dask                    2025.9.1\n",
      "dask-glm                0.3.2\n",
      "dask-ml                 2025.1.0\n",
      "debugpy                 1.8.16\n",
      "decorator               5.2.1\n",
      "distributed             2025.9.1\n",
      "exceptiongroup          1.3.0\n",
      "executing               2.2.1\n",
      "fonttools               4.59.2\n",
      "frozenlist              1.7.0\n",
      "fsspec                  2025.9.0\n",
      "geopandas               1.1.1\n",
      "idna                    3.10\n",
      "importlib_metadata      8.7.0\n",
      "ipykernel               6.30.1\n",
      "ipython                 9.5.0\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "Jinja2                  3.1.6\n",
      "joblib                  1.5.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.8.1\n",
      "kiwisolver              1.4.9\n",
      "llvmlite                0.44.0\n",
      "locket                  1.0.0\n",
      "lxml                    6.0.1\n",
      "MarkupSafe              3.0.2\n",
      "matplotlib              3.10.6\n",
      "matplotlib-inline       0.1.7\n",
      "mllib                   1.0.0a2\n",
      "msgpack                 1.1.1\n",
      "multidict               6.6.4\n",
      "multipledispatch        1.0.0\n",
      "nest_asyncio            1.6.0\n",
      "numba                   0.61.2\n",
      "numpy                   2.2.6\n",
      "packaging               25.0\n",
      "pandas                  2.3.2\n",
      "parso                   0.8.5\n",
      "partd                   1.4.2\n",
      "pexpect                 4.9.0\n",
      "pickleshare             0.7.5\n",
      "pillow                  11.3.0\n",
      "pip                     25.2\n",
      "platformdirs            4.4.0\n",
      "prompt_toolkit          3.0.52\n",
      "propcache               0.3.2\n",
      "psutil                  7.0.0\n",
      "ptyprocess              0.7.0\n",
      "pure_eval               0.2.3\n",
      "py4j                    0.10.9.5\n",
      "pyarrow                 21.0.0\n",
      "Pygments                2.19.2\n",
      "pyogrio                 0.11.1\n",
      "pyparsing               3.2.4\n",
      "pyproj                  3.7.2\n",
      "pyspark                 3.3.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2025.2\n",
      "PyYAML                  6.0.2\n",
      "pyzmq                   27.1.0\n",
      "requests                2.32.5\n",
      "scikit-learn            1.7.2\n",
      "scipy                   1.16.2\n",
      "seaborn                 0.13.2\n",
      "setuptools              80.9.0\n",
      "shapely                 2.1.2\n",
      "six                     1.17.0\n",
      "sortedcontainers        2.4.0\n",
      "sparse                  0.17.0\n",
      "stack_data              0.6.3\n",
      "tblib                   3.1.0\n",
      "threadpoolctl           3.6.0\n",
      "toolz                   1.0.0\n",
      "tornado                 6.5.2\n",
      "traitlets               5.14.3\n",
      "typing_extensions       4.15.0\n",
      "tzdata                  2025.2\n",
      "urllib3                 2.5.0\n",
      "wcwidth                 0.2.13\n",
      "wheel                   0.45.1\n",
      "yarl                    1.20.1\n",
      "zict                    3.0.0\n",
      "zipp                    3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2aa25-4de6-4652-b6ce-c4c45f921d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE SPARK MARKET BASKET ANALYSIS NOTEBOOK\n",
    "# For online_retail_II.xlsx (2 sheets, ~1.6M records)\n",
    "\n",
    "# Install required packages (run this cell first)\n",
    "!pip install pyspark pandas openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6cfdebb-b565-41e4-a20a-3f742cebf850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ INITIALIZING SPARK SESSION\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/28 19:48:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "‚úì Spark session initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize Spark\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize Spark session for notebook environment\"\"\"\n",
    "    print(\"üîÑ INITIALIZING SPARK SESSION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MarketBasketAnalysis\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"‚úì Spark session initialized\")\n",
    "    return spark\n",
    "\n",
    "# Initialize Spark\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db86fe3-5e08-40cd-89df-b704f3a1e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ LOADING CSV FILE\n",
      "----------------------------------------\n",
      "Loading online_retail_II.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Records loaded: 1,067,371\n",
      "‚úì Columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Date range: 2009-12-01 07:45:00 to 2011-12-09 12:50:00\n",
      "\n",
      "üìä SAMPLE DATA:\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   489434|    85048|15CM CHRISTMAS GL...|      12|2009-12-01 07:45:00|     6.95|   13085.0|United Kingdom|\n",
      "|   489434|   79323P|  PINK CHERRY LIGHTS|      12|2009-12-01 07:45:00|     6.75|   13085.0|United Kingdom|\n",
      "|   489434|   79323W| WHITE CHERRY LIGHTS|      12|2009-12-01 07:45:00|     6.75|   13085.0|United Kingdom|\n",
      "|   489434|    22041|\"RECORD FRAME 7\"\"...|      48|2009-12-01 07:45:00|      2.1|   13085.0|United Kingdom|\n",
      "|   489434|    21232|STRAWBERRY CERAMI...|      24|2009-12-01 07:45:00|     1.25|   13085.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1,067,371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 3: Load CSV File\n",
    "def load_csv_data(file_path):\n",
    "    \"\"\"Load data from online_retail_II.csv\"\"\"\n",
    "    print(\"üìÇ LOADING CSV FILE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Define schema for better performance\n",
    "    custom_schema = StructType([\n",
    "        StructField(\"Invoice\", StringType(), True),\n",
    "        StructField(\"StockCode\", StringType(), True),\n",
    "        StructField(\"Description\", StringType(), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"Customer ID\", IntegerType(), True),\n",
    "        StructField(\"Country\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Load CSV with inferred schema (more flexible)\n",
    "    print(\"Loading online_retail_II.csv...\")\n",
    "    df_raw = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    # Rename columns to match our expected format\n",
    "    df_raw = df_raw \\\n",
    "        .withColumnRenamed(\"Invoice\", \"InvoiceNo\") \\\n",
    "        .withColumnRenamed(\"Price\", \"UnitPrice\") \\\n",
    "        .withColumnRenamed(\"Customer ID\", \"CustomerID\")\n",
    "    \n",
    "    original_count = df_raw.count()\n",
    "    print(f\"‚úì Records loaded: {original_count:,}\")\n",
    "    \n",
    "    # Show basic info\n",
    "    print(f\"‚úì Columns: {', '.join(df_raw.columns)}\")\n",
    "    \n",
    "    # Show date range\n",
    "    date_range = df_raw.agg(\n",
    "        min(\"InvoiceDate\").alias(\"min_date\"),\n",
    "        max(\"InvoiceDate\").alias(\"max_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"‚úì Date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "# Load the data\n",
    "file_path = \"online_retail_II.csv\"  # Make sure this file is in your notebook directory\n",
    "df_raw = load_csv_data(file_path)\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüìä SAMPLE DATA:\")\n",
    "df_raw.show(5)\n",
    "print(f\"Total records: {df_raw.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be00c44-049d-488b-9290-f5d0ff1fa89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ COMPREHENSIVE DATA CLEANING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 1,067,371 records\n",
      "\n",
      "4.1 DATA QUALITY ANALYSIS\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "  Description: 4,382 (0.41%)\n",
      "  CustomerID: 243,007 (22.77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data quality issues:\n",
      "  Invalid quantities: 22,950\n",
      "  Invalid prices: 6,207\n",
      "  Cancelled invoices: 19,494\n",
      "\n",
      "4.2 APPLYING CLEANING RULES\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Removed cancellations: 19,494 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Removed invalid data: 6,207 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Removed missing values: 236,121 records\n",
      "‚úì Standardized descriptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Removed non-product items: 19,475 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Removed duplicates: 25,559 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.3 CLEANING RESULTS\n",
      "----------------------------------------\n",
      "Original records: 1,067,371\n",
      "Cleaned records: 760,515\n",
      "Records removed: 306,856\n",
      "Retention rate: 71.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FINAL METRICS:\n",
      "  Transactions: 36,722\n",
      "  Products: 4,503\n",
      "  Customers: 5,866\n",
      "  Avg Quantity: 13.52\n",
      "  Avg Price: $3.12\n",
      "  Total Quantity Sold: 10,283,096\n",
      "\n",
      "‚úÖ CLEANED DATA SAMPLE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   489446|    21671|RED SPOT CERAMIC ...|      12|2009-12-01 10:06:00|     1.25|   13758.0|United Kingdom|\n",
      "|   489594|    21232|STRAWBERRY CERAMI...|      12|2009-12-01 14:19:00|     1.25|   15005.0|United Kingdom|\n",
      "|   489599|    20711|      JUMBO BAG TOYS|      30|2009-12-01 14:40:00|     1.95|   12758.0|      Portugal|\n",
      "|   489676|    21864|UNION JACK FLAG P...|     120|2009-12-02 09:49:00|     1.69|   13777.0|United Kingdom|\n",
      "|   489679|    84371|     BIG PINK POODLE|       1|2009-12-02 10:00:00|    19.95|   16163.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Cleaning Pipeline\n",
    "def comprehensive_cleaning(df_raw):\n",
    "    \"\"\"Complete data cleaning pipeline\"\"\"\n",
    "    print(\"\\nüßπ COMPREHENSIVE DATA CLEANING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    original_count = df_raw.count()\n",
    "    print(f\"Starting with {original_count:,} records\")\n",
    "    \n",
    "    # Data Quality Analysis\n",
    "    print(\"\\n4.1 DATA QUALITY ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_analysis = df_raw.select([\n",
    "        count(when(isnull(c), c)).alias(c) for c in df_raw.columns\n",
    "    ]).collect()[0]\n",
    "    \n",
    "    print(\"Missing values:\")\n",
    "    for col_name in df_raw.columns:\n",
    "        missing_count = missing_analysis[col_name]\n",
    "        if missing_count > 0:\n",
    "            print(f\"  {col_name}: {missing_count:,} ({missing_count/original_count*100:.2f}%)\")\n",
    "    \n",
    "    # Data issues\n",
    "    data_issues = df_raw.agg(\n",
    "        count(when(col(\"Quantity\") <= 0, True)).alias(\"invalid_quantity\"),\n",
    "        count(when(col(\"UnitPrice\") <= 0, True)).alias(\"invalid_price\"),\n",
    "        count(when(col(\"InvoiceNo\").startswith(\"C\"), True)).alias(\"cancelled_invoices\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"\\nData quality issues:\")\n",
    "    print(f\"  Invalid quantities: {data_issues['invalid_quantity']:,}\")\n",
    "    print(f\"  Invalid prices: {data_issues['invalid_price']:,}\")\n",
    "    print(f\"  Cancelled invoices: {data_issues['cancelled_invoices']:,}\")\n",
    "    \n",
    "    # Cleaning Steps\n",
    "    print(\"\\n4.2 APPLYING CLEANING RULES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_clean = df_raw\n",
    "    \n",
    "    # Step 1: Remove cancelled invoices\n",
    "    df_clean = df_clean.filter(~col(\"InvoiceNo\").startswith(\"C\"))\n",
    "    cancelled_count = original_count - df_clean.count()\n",
    "    print(f\"‚úì Removed cancellations: {cancelled_count:,} records\")\n",
    "    \n",
    "    # Step 2: Remove invalid quantities and prices\n",
    "    df_clean = df_clean.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
    "    invalid_count = original_count - cancelled_count - df_clean.count()\n",
    "    print(f\"‚úì Removed invalid data: {invalid_count:,} records\")\n",
    "    \n",
    "    # Step 3: Handle missing values\n",
    "    df_clean = df_clean.filter(col(\"CustomerID\").isNotNull() & col(\"Description\").isNotNull())\n",
    "    missing_count = original_count - cancelled_count - invalid_count - df_clean.count()\n",
    "    print(f\"‚úì Removed missing values: {missing_count:,} records\")\n",
    "    \n",
    "    # Step 4: Standardize descriptions\n",
    "    df_clean = df_clean.withColumn(\"Description\", upper(trim(col(\"Description\"))))\n",
    "    print(f\"‚úì Standardized descriptions\")\n",
    "    \n",
    "    # Step 5: Remove POST/non-product items\n",
    "    non_product_keywords = [\"POST\", \"POSTAGE\", \"CARRIAGE\", \"DISCOUNT\", \"FEE\", \"CHARGE\", \"ADJUST\", \"BANK\", \"CREDIT\", \"GIFT\"]\n",
    "    condition = ~col(\"Description\").rlike(\"|\".join(non_product_keywords))\n",
    "    df_clean = df_clean.filter(condition)\n",
    "    non_product_count = original_count - cancelled_count - invalid_count - missing_count - df_clean.count()\n",
    "    print(f\"‚úì Removed non-product items: {non_product_count:,} records\")\n",
    "    \n",
    "    # Step 6: Remove duplicates\n",
    "    initial_count = df_clean.count()\n",
    "    df_clean = df_clean.dropDuplicates()\n",
    "    duplicate_count = initial_count - df_clean.count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"‚úì Removed duplicates: {duplicate_count:,} records\")\n",
    "    \n",
    "    # Post-cleaning analysis\n",
    "    final_count = df_clean.count()\n",
    "    retention_rate = (final_count / original_count) * 100\n",
    "    \n",
    "    print(f\"\\n4.3 CLEANING RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Original records: {original_count:,}\")\n",
    "    print(f\"Cleaned records: {final_count:,}\")\n",
    "    print(f\"Records removed: {original_count - final_count:,}\")\n",
    "    print(f\"Retention rate: {retention_rate:.2f}%\")\n",
    "    \n",
    "    # Key metrics\n",
    "    metrics = df_clean.agg(\n",
    "        countDistinct(\"InvoiceNo\").alias(\"transactions\"),\n",
    "        countDistinct(\"StockCode\").alias(\"products\"),\n",
    "        countDistinct(\"CustomerID\").alias(\"customers\"),\n",
    "        mean(\"Quantity\").alias(\"avg_quantity\"),\n",
    "        mean(\"UnitPrice\").alias(\"avg_price\"),\n",
    "        sum(\"Quantity\").alias(\"total_quantity\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìä FINAL METRICS:\")\n",
    "    print(f\"  Transactions: {metrics['transactions']:,}\")\n",
    "    print(f\"  Products: {metrics['products']:,}\")\n",
    "    print(f\"  Customers: {metrics['customers']:,}\")\n",
    "    print(f\"  Avg Quantity: {metrics['avg_quantity']:.2f}\")\n",
    "    print(f\"  Avg Price: ${metrics['avg_price']:.2f}\")\n",
    "    print(f\"  Total Quantity Sold: {metrics['total_quantity']:,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Run cleaning\n",
    "df_clean = comprehensive_cleaning(df_raw)\n",
    "\n",
    "# Show cleaned data sample\n",
    "print(\"\\n‚úÖ CLEANED DATA SAMPLE:\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "478da2a7-9128-4674-b187-92b82feaebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõí PREPARING TRANSACTION DATA\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 36,722 transaction baskets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä BASKET STATISTICS:\n",
      "  Average size: 20.71 items\n",
      "  Std deviation: 22.54 items\n",
      "  Min size: 1 items\n",
      "  Max size: 533 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  95th percentile: 61 items\n",
      "\n",
      "üéØ SAMPLE TRANSACTIONS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Basket 1: 8 items - First 3: ['22041', '79323W', '21523']\n",
      "  Basket 2: 4 items - First 3: ['22353', '22349', '22350']\n",
      "  Basket 3: 17 items - First 3: ['21252', '21411', '21033']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 5: Prepare Transactions for FP-Growth\n",
    "def prepare_transactions(df_clean):\n",
    "    \"\"\"Prepare transaction baskets for FP-Growth\"\"\"\n",
    "    print(\"\\nüõí PREPARING TRANSACTION DATA\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create transaction baskets\n",
    "    transactions_df = df_clean.groupBy(\"InvoiceNo\") \\\n",
    "        .agg(collect_list(\"StockCode\").alias(\"items\")) \\\n",
    "        .filter(size(col(\"items\")) > 0)\n",
    "    \n",
    "    # Add basket size\n",
    "    transactions_df = transactions_df.withColumn(\"basket_size\", size(col(\"items\")))\n",
    "    \n",
    "    transaction_count = transactions_df.count()\n",
    "    print(f\"‚úì Created {transaction_count:,} transaction baskets\")\n",
    "    \n",
    "    # Basket statistics\n",
    "    basket_stats = transactions_df.select(\n",
    "        mean(\"basket_size\").alias(\"avg_size\"),\n",
    "        stddev(\"basket_size\").alias(\"std_size\"),\n",
    "        min(\"basket_size\").alias(\"min_size\"),\n",
    "        max(\"basket_size\").alias(\"max_size\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìä BASKET STATISTICS:\")\n",
    "    print(f\"  Average size: {basket_stats['avg_size']:.2f} items\")\n",
    "    print(f\"  Std deviation: {basket_stats['std_size']:.2f} items\")\n",
    "    print(f\"  Min size: {basket_stats['min_size']} items\")\n",
    "    print(f\"  Max size: {basket_stats['max_size']} items\")\n",
    "    \n",
    "    # Show basket size distribution\n",
    "    basket_sizes = transactions_df.select(\"basket_size\").toPandas()\n",
    "    print(f\"  95th percentile: {basket_sizes['basket_size'].quantile(0.95):.0f} items\")\n",
    "    \n",
    "    # Show sample transactions\n",
    "    print(f\"\\nüéØ SAMPLE TRANSACTIONS:\")\n",
    "    samples = transactions_df.limit(3).collect()\n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"  Basket {i+1}: {sample['basket_size']} items - First 3: {sample['items'][:3]}\")\n",
    "    \n",
    "    return transactions_df\n",
    "\n",
    "# Prepare transactions\n",
    "transactions_df = prepare_transactions(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3a2f56-da11-443d-9a60-fb2cd38ac74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç RUNNING FP-GROWTH ANALYSIS\n",
      "----------------------------------------\n",
      "Minimum Support: 0.005\n",
      "Minimum Confidence: 0.4\n",
      "Training FP-Growth model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/28 20:09:06 ERROR Executor: Exception in task 1.0 in stage 107.0 (TID 94)\n",
      "org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/11/28 20:09:06 ERROR Executor: Exception in task 0.0 in stage 107.0 (TID 93)\n",
      "org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(84997A, 22319, 21931, 22122, 85129A, 85123A, 21912, 47591A, 21705, 21791, 21244, 21584, 21790, 21821, 20971, 21790, 20972, 21837, 22130, 22335, 21913, 16207A, 84951A, 84997B, 21791, 84791, 21252, 20711, 20675).\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/11/28 20:09:06 WARN TaskSetManager: Lost task 1.0 in stage 107.0 (TID 94) (jupyter-km4886 executor driver): org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/11/28 20:09:06 ERROR TaskSetManager: Task 1 in stage 107.0 failed 1 times; aborting job\n",
      "25/11/28 20:09:06 WARN TaskSetManager: Lost task 0.0 in stage 107.0 (TID 93) (jupyter-km4886 executor driver): org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(84997A, 22319, 21931, 22122, 85129A, 85123A, 21912, 47591A, 21705, 21791, 21244, 21584, 21790, 21821, 20971, 21790, 20972, 21837, 22130, 22335, 21913, 16207A, 84951A, 84997B, 21791, 84791, 21252, 20711, 20675).\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/11/28 20:09:06 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 107.0 failed 1 times, most recent failure: Lost task 1.0 in stage 107.0 (TID 94) (jupyter-km4886 executor driver): org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.genFreqItems(FPGrowth.scala:254)\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.run(FPGrowth.scala:219)\n",
      "\tat org.apache.spark.ml.fpm.FPGrowth.$anonfun$genericFit$1(FPGrowth.scala:180)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.fpm.FPGrowth.genericFit(FPGrowth.scala:162)\n",
      "\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:159)\n",
      "\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:129)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n",
      "\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o221.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 107.0 failed 1 times, most recent failure: Lost task 1.0 in stage 107.0 (TID 94) (jupyter-km4886 executor driver): org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.mllib.fpm.FPGrowth.genFreqItems(FPGrowth.scala:254)\n\tat org.apache.spark.mllib.fpm.FPGrowth.run(FPGrowth.scala:219)\n\tat org.apache.spark.ml.fpm.FPGrowth.$anonfun$genericFit$1(FPGrowth.scala:180)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.fpm.FPGrowth.genericFit(FPGrowth.scala:162)\n\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:159)\n\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:129)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, frequent_itemsets, association_rules\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Run FP-Growth\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m model, frequent_itemsets, association_rules = \u001b[43mrun_fp_growth_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransactions_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Show sample results\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç SAMPLE RESULTS:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mrun_fp_growth_analysis\u001b[39m\u001b[34m(transactions_df, min_support, min_confidence)\u001b[39m\n\u001b[32m     12\u001b[39m fp_growth = FPGrowth(\n\u001b[32m     13\u001b[39m     itemsCol=\u001b[33m\"\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     14\u001b[39m     minSupport=min_support, \n\u001b[32m     15\u001b[39m     minConfidence=min_confidence,\n\u001b[32m     16\u001b[39m     numPartitions=\u001b[32m8\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining FP-Growth model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mfp_growth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransactions_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Get results\u001b[39;00m\n\u001b[32m     23\u001b[39m frequent_itemsets = model.freqItemsets\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/csgyc-6513-fall/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/csgyc-6513-fall/lib/python3.12/site-packages/pyspark/ml/wrapper.py:383\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/csgyc-6513-fall/lib/python3.12/site-packages/pyspark/ml/wrapper.py:380\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/csgyc-6513-fall/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1315\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1317\u001b[39m     args_command +\\\n\u001b[32m   1318\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1320\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1325\u001b[39m     temp_arg._detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/csgyc-6513-fall/lib/python3.12/site-packages/pyspark/sql/utils.py:190\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    192\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/csgyc-6513-fall/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o221.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 107.0 failed 1 times, most recent failure: Lost task 1.0 in stage 107.0 (TID 94) (jupyter-km4886 executor driver): org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.mllib.fpm.FPGrowth.genFreqItems(FPGrowth.scala:254)\n\tat org.apache.spark.mllib.fpm.FPGrowth.run(FPGrowth.scala:219)\n\tat org.apache.spark.ml.fpm.FPGrowth.$anonfun$genericFit$1(FPGrowth.scala:180)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.fpm.FPGrowth.genericFit(FPGrowth.scala:162)\n\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:159)\n\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:129)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Items in a transaction must be unique but got WrappedArray(20967, 90195B, 20971, 84032A, 22125, 21482, 84031B, 22086, 21058, 21063, 21930, 20870, 21100, 21231, 20970, 84582, 90121B, 21484, 84327A, 90175A, 84032B, 90062, 20972, 84997C, 22125, 84997D).\n\tat org.apache.spark.mllib.fpm.FPGrowth.$anonfun$genFreqItems$1(FPGrowth.scala:249)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Run FP-Growth\n",
    "def run_fp_growth_analysis(transactions_df, min_support=0.005, min_confidence=0.4):\n",
    "    \"\"\"Run FP-Growth algorithm and generate association rules\"\"\"\n",
    "    print(f\"\\nüîç RUNNING FP-GROWTH ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Minimum Support: {min_support}\")\n",
    "    print(f\"Minimum Confidence: {min_confidence}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize and run FP-Growth\n",
    "    fp_growth = FPGrowth(\n",
    "        itemsCol=\"items\", \n",
    "        minSupport=min_support, \n",
    "        minConfidence=min_confidence,\n",
    "        numPartitions=8\n",
    "    )\n",
    "    \n",
    "    print(\"Training FP-Growth model...\")\n",
    "    model = fp_growth.fit(transactions_df)\n",
    "    \n",
    "    # Get results\n",
    "    frequent_itemsets = model.freqItemsets\n",
    "    association_rules = model.associationRules\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úì FP-Growth completed in {processing_time:.2f} seconds\")\n",
    "    print(f\"‚úì Frequent itemsets: {frequent_itemsets.count():,}\")\n",
    "    print(f\"‚úì Association rules: {association_rules.count():,}\")\n",
    "    \n",
    "    return model, frequent_itemsets, association_rules\n",
    "\n",
    "# Run FP-Growth\n",
    "model, frequent_itemsets, association_rules = run_fp_growth_analysis(transactions_df)\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nüîç SAMPLE RESULTS:\")\n",
    "print(\"Frequent Itemsets sample:\")\n",
    "frequent_itemsets.show(5)\n",
    "print(\"Association Rules sample:\")\n",
    "association_rules.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5017b-3813-46c9-9c28-eec679f26a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analyze Results\n",
    "def analyze_results(frequent_itemsets, association_rules, transactions_count):\n",
    "    \"\"\"Analyze and display FP-Growth results\"\"\"\n",
    "    print(\"\\nüìä ANALYZING RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    freq_itemsets_pd = frequent_itemsets.orderBy(desc(\"freq\")).limit(20).toPandas()\n",
    "    rules_pd = association_rules.orderBy(desc(\"confidence\")).limit(20).toPandas()\n",
    "    \n",
    "    # Calculate support for itemsets\n",
    "    freq_itemsets_pd['support'] = freq_itemsets_pd['freq'] / transactions_count\n",
    "    \n",
    "    print(\"üèÜ TOP 10 FREQUENT ITEMSETS:\")\n",
    "    for i, row in freq_itemsets_pd.head(10).iterrows():\n",
    "        items_str = str(row['items'])[:80] + \"...\" if len(str(row['items'])) > 80 else str(row['items'])\n",
    "        print(f\"  {i+1:2d}. Support: {row['support']:.4f} - Items: {items_str}\")\n",
    "    \n",
    "    print(f\"\\nüéØ TOP 10 ASSOCIATION RULES:\")\n",
    "    for i, row in rules_pd.head(10).iterrows():\n",
    "        support = row['freq'] / transactions_count\n",
    "        antecedent_str = str(row['antecedent'])[:50] + \"...\" if len(str(row['antecedent'])) > 50 else str(row['antecedent'])\n",
    "        consequent_str = str(row['consequent'])[:50] + \"...\" if len(str(row['consequent'])) > 50 else str(row['consequent'])\n",
    "        print(f\"  {i+1:2d}. {antecedent_str} ‚Üí {consequent_str}\")\n",
    "        print(f\"      Confidence: {row['confidence']:.3f}, Support: {support:.4f}\")\n",
    "    \n",
    "    # Rule quality analysis\n",
    "    if len(rules_pd) > 0:\n",
    "        print(f\"\\nüìà RULE QUALITY SUMMARY:\")\n",
    "        print(f\"  Total rules: {len(rules_pd):,}\")\n",
    "        print(f\"  Avg confidence: {rules_pd['confidence'].mean():.3f}\")\n",
    "        print(f\"  Max confidence: {rules_pd['confidence'].max():.3f}\")\n",
    "        print(f\"  High confidence rules (>0.7): {len(rules_pd[rules_pd['confidence'] > 0.7])}\")\n",
    "        print(f\"  Medium confidence rules (>0.5): {len(rules_pd[rules_pd['confidence'] > 0.5])}\")\n",
    "        print(f\"  Avg rule length: {rules_pd['antecedent'].apply(len).mean() + rules_pd['consequent'].apply(len).mean():.1f} items\")\n",
    "    \n",
    "    return freq_itemsets_pd, rules_pd\n",
    "\n",
    "# Analyze results\n",
    "transaction_count = transactions_df.count()\n",
    "freq_itemsets_pd, rules_pd = analyze_results(frequent_itemsets, association_rules, transaction_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592d1f5-9c96-4670-9fe0-96324ffa88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create Visualizations\n",
    "def create_visualizations(freq_itemsets_pd, rules_pd, df_clean, transactions_df):\n",
    "    \"\"\"Create visualizations for the analysis\"\"\"\n",
    "    print(\"\\nüìà CREATING VISUALIZATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Set up plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Market Basket Analysis - Online Retail II', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Top frequent itemsets by support\n",
    "    if len(freq_itemsets_pd) > 0:\n",
    "        top_itemsets = freq_itemsets_pd.head(8)\n",
    "        # Create shortened labels\n",
    "        labels = []\n",
    "        for items in top_itemsets['items']:\n",
    "            if len(items) == 1:\n",
    "                labels.append(f\"Single: {items[0]}\")\n",
    "            else:\n",
    "                labels.append(f\"Combo: {len(items)} items\")\n",
    "        \n",
    "        axes[0, 0].barh(range(len(top_itemsets)), top_itemsets['support'])\n",
    "        axes[0, 0].set_yticks(range(len(top_itemsets)))\n",
    "        axes[0, 0].set_yticklabels(labels)\n",
    "        axes[0, 0].set_title('Top 8 Frequent Itemsets by Support', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Support')\n",
    "        axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Confidence distribution of rules\n",
    "    if len(rules_pd) > 0:\n",
    "        axes[0, 1].hist(rules_pd['confidence'], bins=20, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "        axes[0, 1].set_title('Distribution of Rule Confidence', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Confidence')\n",
    "        axes[0, 1].set_ylabel('Number of Rules')\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Top rules by confidence\n",
    "    if len(rules_pd) > 0:\n",
    "        top_rules = rules_pd.head(6)\n",
    "        y_pos = np.arange(len(top_rules))\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(top_rules)))\n",
    "        \n",
    "        bars = axes[1, 0].barh(y_pos, top_rules['confidence'], color=colors)\n",
    "        axes[1, 0].set_yticks(y_pos)\n",
    "        axes[1, 0].set_yticklabels([f\"Rule {i+1}\" for i in range(len(top_rules))])\n",
    "        axes[1, 0].set_title('Top 6 Rules by Confidence', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Confidence')\n",
    "        axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add confidence values on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            axes[1, 0].text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                           f'{width:.3f}', ha='left', va='center')\n",
    "    \n",
    "    # Plot 4: Basket size distribution\n",
    "    basket_sizes_pd = transactions_df.select(\"basket_size\").toPandas()\n",
    "    axes[1, 1].hist(basket_sizes_pd['basket_size'], bins=50, alpha=0.7, \n",
    "                   edgecolor='black', color='lightgreen')\n",
    "    axes[1, 1].set_title('Distribution of Basket Sizes', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Items per Basket')\n",
    "    axes[1, 1].set_ylabel('Frequency (log scale)')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional visualization: Support vs Confidence scatter plot\n",
    "    if len(rules_pd) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(rules_pd['freq']/transaction_count, rules_pd['confidence'], \n",
    "                            alpha=0.6, c=rules_pd['confidence'], cmap='viridis')\n",
    "        plt.colorbar(scatter, label='Confidence')\n",
    "        plt.xlabel('Support')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.title('Support vs Confidence for Association Rules', fontweight='bold')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"‚úì Visualizations created successfully\")\n",
    "\n",
    "# Create visualizations\n",
    "create_visualizations(freq_itemsets_pd, rules_pd, df_clean, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284dc52-b62f-462b-8ba5-67746b8127bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "def save_results(df_clean, frequent_itemsets, association_rules, transactions_df):\n",
    "    \"\"\"Save all results to files\"\"\"\n",
    "    print(\"\\nüíæ SAVING RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"csv_results\", exist_ok=True)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df_clean.write.mode(\"overwrite\").parquet(\"csv_results/cleaned_data\")\n",
    "    df_clean.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/cleaned_data_csv\")\n",
    "    \n",
    "    # Save frequent itemsets\n",
    "    frequent_itemsets.write.mode(\"overwrite\").parquet(\"csv_results/frequent_itemsets\")\n",
    "    frequent_itemsets.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/frequent_itemsets_csv\")\n",
    "    \n",
    "    # Save association rules\n",
    "    association_rules.write.mode(\"overwrite\").parquet(\"csv_results/association_rules\")\n",
    "    association_rules.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/association_rules_csv\")\n",
    "    \n",
    "    # Save transaction data\n",
    "    transactions_df.write.mode(\"overwrite\").parquet(\"csv_results/transactions\")\n",
    "    \n",
    "    # Save sample data for quick inspection\n",
    "    df_clean.limit(10000).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/sample_data\")\n",
    "    \n",
    "    print(\"‚úì All results saved to 'csv_results' folder:\")\n",
    "    print(\"  - cleaned_data/ (Parquet + CSV)\")\n",
    "    print(\"  - frequent_itemsets/ (Parquet + CSV)\")\n",
    "    print(\"  - association_rules/ (Parquet + CSV)\")\n",
    "    print(\"  - transactions/ (Parquet)\")\n",
    "    print(\"  - sample_data/ (CSV)\")\n",
    "\n",
    "# Save results\n",
    "save_results(df_clean, frequent_itemsets, association_rules, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaa2e2-0d7d-41c9-ba0f-22dbd35841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Generate Business Insights\n",
    "def generate_business_insights(rules_pd, freq_itemsets_pd, top_n=10):\n",
    "    \"\"\"Generate actionable business insights\"\"\"\n",
    "    print(\"\\nüí° BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"üéØ TOP CROSS-SELLING OPPORTUNITIES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, row in rules_pd.head(top_n).iterrows():\n",
    "        print(f\"\\n{i+1}. WHEN customers buy: {row['antecedent']}\")\n",
    "        print(f\"   THEY ALSO buy: {row['consequent']}\")\n",
    "        print(f\"   Confidence: {row['confidence']:.1%}\")\n",
    "        \n",
    "        # Business recommendation based on confidence\n",
    "        if row['confidence'] >= 0.7:\n",
    "            recommendation = \"üí™ STRONG BUNDLE - Create product bundles and recommend together\"\n",
    "        elif row['confidence'] >= 0.5:\n",
    "            recommendation = \"üëç MODERATE OPPORTUNITY - Cross-sell promotions and suggestions\"\n",
    "        else:\n",
    "            recommendation = \"üëÄ WEAK ASSOCIATION - Monitor and test with discounts\"\n",
    "        \n",
    "        print(f\"   üí° ACTION: {recommendation}\")\n",
    "    \n",
    "    print(f\"\\nüì¶ POPULAR PRODUCT COMBINATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, row in freq_itemsets_pd.head(5).iterrows():\n",
    "        if len(row['items']) == 1:\n",
    "            print(f\"  {i+1}. Single popular item: {row['items']} (Support: {row['support']:.2%})\")\n",
    "        else:\n",
    "            print(f\"  {i+1}. Popular combination: {row['items']} (Support: {row['support']:.2%})\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìà PROJECT SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Total association rules found: {len(rules_pd):,}\")\n",
    "    print(f\"  High-confidence rules (‚â•70%): {len(rules_pd[rules_pd['confidence'] >= 0.7])}\")\n",
    "    print(f\"  Medium-confidence rules (‚â•50%): {len(rules_pd[rules_pd['confidence'] >= 0.5])}\")\n",
    "    print(f\"  Average rule confidence: {rules_pd['confidence'].mean():.1%}\")\n",
    "\n",
    "# Generate insights\n",
    "generate_business_insights(rules_pd, freq_itemsets_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fef083-d9ce-471d-86f8-069a2f928a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Performance Summary and Cleanup\n",
    "print(\"\\n‚è±Ô∏è PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate some basic stats\n",
    "total_records = df_raw.count()\n",
    "cleaned_records = df_clean.count()\n",
    "transactions_count = transactions_df.count()\n",
    "itemsets_count = frequent_itemsets.count()\n",
    "rules_count = association_rules.count()\n",
    "\n",
    "print(f\"üìä DATASET STATISTICS:\")\n",
    "print(f\"  Original records: {total_records:,}\")\n",
    "print(f\"  Cleaned records: {cleaned_records:,}\")\n",
    "print(f\"  Transaction baskets: {transactions_count:,}\")\n",
    "print(f\"  Frequent itemsets: {itemsets_count:,}\")\n",
    "print(f\"  Association rules: {rules_count:,}\")\n",
    "\n",
    "print(f\"\\nüéØ BUSINESS IMPACT:\")\n",
    "print(f\"  Cross-selling opportunities: {rules_count:,} rules\")\n",
    "print(f\"  Product combinations: {itemsets_count:,} patterns\")\n",
    "print(f\"  Customer transactions analyzed: {transactions_count:,}\")\n",
    "\n",
    "print(f\"\\nüîö CLEANUP\")\n",
    "print(\"-\" * 40)\n",
    "spark.stop()\n",
    "print(\"‚úì Spark session stopped\")\n",
    "print(\"\\n‚úÖ NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"üíæ Results saved in 'csv_results/' folder\")\n",
    "print(\"üìà Visualizations generated\")\n",
    "print(\"üí° Business insights ready for implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:csgyc-6513-fall]",
   "language": "python",
   "name": "conda-env-csgyc-6513-fall-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
