{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76495bf0-f6e3-40dd-818f-9b025bc7cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE SPARK MBA NOTEBOOK ===\n",
      "Dataset: online_retail_II.xlsx (2 sheets)\n",
      "Expected records: ~1.1 million\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import all libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== COMPLETE SPARK MBA NOTEBOOK ===\")\n",
    "print(\"Dataset: online_retail_II.xlsx (2 sheets)\")\n",
    "print(\"Expected records: ~1.1 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4c40e1-bb3d-4217-9fb8-b5277331cfc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "aiohappyeyeballs        2.6.1\n",
      "aiohttp                 3.12.15\n",
      "aiosignal               1.4.0\n",
      "asttokens               3.0.0\n",
      "attrs                   25.3.0\n",
      "certifi                 2025.8.3\n",
      "charset-normalizer      3.4.3\n",
      "click                   8.2.1\n",
      "cloudpickle             3.1.1\n",
      "comm                    0.2.3\n",
      "complete                0.0.1\n",
      "contourpy               1.3.3\n",
      "cycler                  0.12.1\n",
      "dask                    2025.9.1\n",
      "dask-glm                0.3.2\n",
      "dask-ml                 2025.1.0\n",
      "debugpy                 1.8.16\n",
      "decorator               5.2.1\n",
      "distributed             2025.9.1\n",
      "exceptiongroup          1.3.0\n",
      "executing               2.2.1\n",
      "fonttools               4.59.2\n",
      "frozenlist              1.7.0\n",
      "fsspec                  2025.9.0\n",
      "geopandas               1.1.1\n",
      "idna                    3.10\n",
      "importlib_metadata      8.7.0\n",
      "ipykernel               6.30.1\n",
      "ipython                 9.5.0\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "Jinja2                  3.1.6\n",
      "joblib                  1.5.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.8.1\n",
      "kiwisolver              1.4.9\n",
      "llvmlite                0.44.0\n",
      "locket                  1.0.0\n",
      "lxml                    6.0.1\n",
      "MarkupSafe              3.0.2\n",
      "matplotlib              3.10.6\n",
      "matplotlib-inline       0.1.7\n",
      "mllib                   1.0.0a2\n",
      "msgpack                 1.1.1\n",
      "multidict               6.6.4\n",
      "multipledispatch        1.0.0\n",
      "nest_asyncio            1.6.0\n",
      "numba                   0.61.2\n",
      "numpy                   2.2.6\n",
      "packaging               25.0\n",
      "pandas                  2.3.2\n",
      "parso                   0.8.5\n",
      "partd                   1.4.2\n",
      "pexpect                 4.9.0\n",
      "pickleshare             0.7.5\n",
      "pillow                  11.3.0\n",
      "pip                     25.2\n",
      "platformdirs            4.4.0\n",
      "prompt_toolkit          3.0.52\n",
      "propcache               0.3.2\n",
      "psutil                  7.0.0\n",
      "ptyprocess              0.7.0\n",
      "pure_eval               0.2.3\n",
      "py4j                    0.10.9.5\n",
      "pyarrow                 21.0.0\n",
      "Pygments                2.19.2\n",
      "pyogrio                 0.11.1\n",
      "pyparsing               3.2.4\n",
      "pyproj                  3.7.2\n",
      "pyspark                 3.3.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2025.2\n",
      "PyYAML                  6.0.2\n",
      "pyzmq                   27.1.0\n",
      "requests                2.32.5\n",
      "scikit-learn            1.7.2\n",
      "scipy                   1.16.2\n",
      "seaborn                 0.13.2\n",
      "setuptools              80.9.0\n",
      "shapely                 2.1.2\n",
      "six                     1.17.0\n",
      "sortedcontainers        2.4.0\n",
      "sparse                  0.17.0\n",
      "stack_data              0.6.3\n",
      "tblib                   3.1.0\n",
      "threadpoolctl           3.6.0\n",
      "toolz                   1.0.0\n",
      "tornado                 6.5.2\n",
      "traitlets               5.14.3\n",
      "typing_extensions       4.15.0\n",
      "tzdata                  2025.2\n",
      "urllib3                 2.5.0\n",
      "wcwidth                 0.2.13\n",
      "wheel                   0.45.1\n",
      "yarl                    1.20.1\n",
      "zict                    3.0.0\n",
      "zipp                    3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cfdebb-b565-41e4-a20a-3f742cebf850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ INITIALIZING SPARK SESSION\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/28 20:37:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "âœ“ Spark session initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize Spark\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize Spark session for notebook environment\"\"\"\n",
    "    print(\"ðŸ”„ INITIALIZING SPARK SESSION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MarketBasketAnalysis\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"âœ“ Spark session initialized\")\n",
    "    return spark\n",
    "\n",
    "# Initialize Spark\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db86fe3-5e08-40cd-89df-b704f3a1e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ LOADING CSV FILE\n",
      "----------------------------------------\n",
      "Loading online_retail_II.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Records loaded: 1,067,371\n",
      "âœ“ Columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Date range: 2009-12-01 07:45:00 to 2011-12-09 12:50:00\n",
      "\n",
      "ðŸ“Š SAMPLE DATA:\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   489434|    85048|15CM CHRISTMAS GL...|      12|2009-12-01 07:45:00|     6.95|   13085.0|United Kingdom|\n",
      "|   489434|   79323P|  PINK CHERRY LIGHTS|      12|2009-12-01 07:45:00|     6.75|   13085.0|United Kingdom|\n",
      "|   489434|   79323W| WHITE CHERRY LIGHTS|      12|2009-12-01 07:45:00|     6.75|   13085.0|United Kingdom|\n",
      "|   489434|    22041|\"RECORD FRAME 7\"\"...|      48|2009-12-01 07:45:00|      2.1|   13085.0|United Kingdom|\n",
      "|   489434|    21232|STRAWBERRY CERAMI...|      24|2009-12-01 07:45:00|     1.25|   13085.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1,067,371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 3: Load CSV File\n",
    "def load_csv_data(file_path):\n",
    "    \"\"\"Load data from online_retail_II.csv\"\"\"\n",
    "    print(\"ðŸ“‚ LOADING CSV FILE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Define schema for better performance\n",
    "    custom_schema = StructType([\n",
    "        StructField(\"Invoice\", StringType(), True),\n",
    "        StructField(\"StockCode\", StringType(), True),\n",
    "        StructField(\"Description\", StringType(), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"Customer ID\", IntegerType(), True),\n",
    "        StructField(\"Country\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Load CSV with inferred schema (more flexible)\n",
    "    print(\"Loading online_retail_II.csv...\")\n",
    "    df_raw = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    # Rename columns to match our expected format\n",
    "    df_raw = df_raw \\\n",
    "        .withColumnRenamed(\"Invoice\", \"InvoiceNo\") \\\n",
    "        .withColumnRenamed(\"Price\", \"UnitPrice\") \\\n",
    "        .withColumnRenamed(\"Customer ID\", \"CustomerID\")\n",
    "    \n",
    "    original_count = df_raw.count()\n",
    "    print(f\"âœ“ Records loaded: {original_count:,}\")\n",
    "    \n",
    "    # Show basic info\n",
    "    print(f\"âœ“ Columns: {', '.join(df_raw.columns)}\")\n",
    "    \n",
    "    # Show date range\n",
    "    date_range = df_raw.agg(\n",
    "        min(\"InvoiceDate\").alias(\"min_date\"),\n",
    "        max(\"InvoiceDate\").alias(\"max_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"âœ“ Date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "# Load the data\n",
    "file_path = \"online_retail_II.csv\"  # Make sure this file is in your notebook directory\n",
    "df_raw = load_csv_data(file_path)\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nðŸ“Š SAMPLE DATA:\")\n",
    "df_raw.show(5)\n",
    "print(f\"Total records: {df_raw.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be00c44-049d-488b-9290-f5d0ff1fa89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¹ COMPREHENSIVE DATA CLEANING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 1,067,371 records\n",
      "\n",
      "4.1 DATA QUALITY ANALYSIS\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "  Description: 4,382 (0.41%)\n",
      "  CustomerID: 243,007 (22.77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data quality issues:\n",
      "  Invalid quantities: 22,950\n",
      "  Invalid prices: 6,207\n",
      "  Cancelled invoices: 19,494\n",
      "\n",
      "4.2 APPLYING CLEANING RULES\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Removed cancellations: 19,494 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Removed invalid data: 6,207 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Removed missing values: 236,121 records\n",
      "âœ“ Standardized descriptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Removed non-product items: 19,475 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Removed duplicates: 25,559 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.3 CLEANING RESULTS\n",
      "----------------------------------------\n",
      "Original records: 1,067,371\n",
      "Cleaned records: 760,515\n",
      "Records removed: 306,856\n",
      "Retention rate: 71.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š FINAL METRICS:\n",
      "  Transactions: 36,722\n",
      "  Products: 4,503\n",
      "  Customers: 5,866\n",
      "  Avg Quantity: 13.52\n",
      "  Avg Price: $3.12\n",
      "  Total Quantity Sold: 10,283,096\n",
      "\n",
      "âœ… CLEANED DATA SAMPLE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   489446|    21671|RED SPOT CERAMIC ...|      12|2009-12-01 10:06:00|     1.25|   13758.0|United Kingdom|\n",
      "|   489594|    21232|STRAWBERRY CERAMI...|      12|2009-12-01 14:19:00|     1.25|   15005.0|United Kingdom|\n",
      "|   489599|    20711|      JUMBO BAG TOYS|      30|2009-12-01 14:40:00|     1.95|   12758.0|      Portugal|\n",
      "|   489676|    21864|UNION JACK FLAG P...|     120|2009-12-02 09:49:00|     1.69|   13777.0|United Kingdom|\n",
      "|   489679|    84371|     BIG PINK POODLE|       1|2009-12-02 10:00:00|    19.95|   16163.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Cleaning Pipeline\n",
    "def comprehensive_cleaning(df_raw):\n",
    "    \"\"\"Complete data cleaning pipeline\"\"\"\n",
    "    print(\"\\nðŸ§¹ COMPREHENSIVE DATA CLEANING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    original_count = df_raw.count()\n",
    "    print(f\"Starting with {original_count:,} records\")\n",
    "    \n",
    "    # Data Quality Analysis\n",
    "    print(\"\\n4.1 DATA QUALITY ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_analysis = df_raw.select([\n",
    "        count(when(isnull(c), c)).alias(c) for c in df_raw.columns\n",
    "    ]).collect()[0]\n",
    "    \n",
    "    print(\"Missing values:\")\n",
    "    for col_name in df_raw.columns:\n",
    "        missing_count = missing_analysis[col_name]\n",
    "        if missing_count > 0:\n",
    "            print(f\"  {col_name}: {missing_count:,} ({missing_count/original_count*100:.2f}%)\")\n",
    "    \n",
    "    # Data issues\n",
    "    data_issues = df_raw.agg(\n",
    "        count(when(col(\"Quantity\") <= 0, True)).alias(\"invalid_quantity\"),\n",
    "        count(when(col(\"UnitPrice\") <= 0, True)).alias(\"invalid_price\"),\n",
    "        count(when(col(\"InvoiceNo\").startswith(\"C\"), True)).alias(\"cancelled_invoices\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"\\nData quality issues:\")\n",
    "    print(f\"  Invalid quantities: {data_issues['invalid_quantity']:,}\")\n",
    "    print(f\"  Invalid prices: {data_issues['invalid_price']:,}\")\n",
    "    print(f\"  Cancelled invoices: {data_issues['cancelled_invoices']:,}\")\n",
    "    \n",
    "    # Cleaning Steps\n",
    "    print(\"\\n4.2 APPLYING CLEANING RULES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_clean = df_raw\n",
    "    \n",
    "    # Step 1: Remove cancelled invoices\n",
    "    df_clean = df_clean.filter(~col(\"InvoiceNo\").startswith(\"C\"))\n",
    "    cancelled_count = original_count - df_clean.count()\n",
    "    print(f\"âœ“ Removed cancellations: {cancelled_count:,} records\")\n",
    "    \n",
    "    # Step 2: Remove invalid quantities and prices\n",
    "    df_clean = df_clean.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
    "    invalid_count = original_count - cancelled_count - df_clean.count()\n",
    "    print(f\"âœ“ Removed invalid data: {invalid_count:,} records\")\n",
    "    \n",
    "    # Step 3: Handle missing values\n",
    "    df_clean = df_clean.filter(col(\"CustomerID\").isNotNull() & col(\"Description\").isNotNull())\n",
    "    missing_count = original_count - cancelled_count - invalid_count - df_clean.count()\n",
    "    print(f\"âœ“ Removed missing values: {missing_count:,} records\")\n",
    "    \n",
    "    # Step 4: Standardize descriptions\n",
    "    df_clean = df_clean.withColumn(\"Description\", upper(trim(col(\"Description\"))))\n",
    "    print(f\"âœ“ Standardized descriptions\")\n",
    "    \n",
    "    # Step 5: Remove POST/non-product items\n",
    "    non_product_keywords = [\"POST\", \"POSTAGE\", \"CARRIAGE\", \"DISCOUNT\", \"FEE\", \"CHARGE\", \"ADJUST\", \"BANK\", \"CREDIT\", \"GIFT\"]\n",
    "    condition = ~col(\"Description\").rlike(\"|\".join(non_product_keywords))\n",
    "    df_clean = df_clean.filter(condition)\n",
    "    non_product_count = original_count - cancelled_count - invalid_count - missing_count - df_clean.count()\n",
    "    print(f\"âœ“ Removed non-product items: {non_product_count:,} records\")\n",
    "    \n",
    "    # Step 6: Remove duplicates\n",
    "    initial_count = df_clean.count()\n",
    "    df_clean = df_clean.dropDuplicates()\n",
    "    duplicate_count = initial_count - df_clean.count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"âœ“ Removed duplicates: {duplicate_count:,} records\")\n",
    "    \n",
    "    # Post-cleaning analysis\n",
    "    final_count = df_clean.count()\n",
    "    retention_rate = (final_count / original_count) * 100\n",
    "    \n",
    "    print(f\"\\n4.3 CLEANING RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Original records: {original_count:,}\")\n",
    "    print(f\"Cleaned records: {final_count:,}\")\n",
    "    print(f\"Records removed: {original_count - final_count:,}\")\n",
    "    print(f\"Retention rate: {retention_rate:.2f}%\")\n",
    "    \n",
    "    # Key metrics\n",
    "    metrics = df_clean.agg(\n",
    "        countDistinct(\"InvoiceNo\").alias(\"transactions\"),\n",
    "        countDistinct(\"StockCode\").alias(\"products\"),\n",
    "        countDistinct(\"CustomerID\").alias(\"customers\"),\n",
    "        mean(\"Quantity\").alias(\"avg_quantity\"),\n",
    "        mean(\"UnitPrice\").alias(\"avg_price\"),\n",
    "        sum(\"Quantity\").alias(\"total_quantity\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\nðŸ“Š FINAL METRICS:\")\n",
    "    print(f\"  Transactions: {metrics['transactions']:,}\")\n",
    "    print(f\"  Products: {metrics['products']:,}\")\n",
    "    print(f\"  Customers: {metrics['customers']:,}\")\n",
    "    print(f\"  Avg Quantity: {metrics['avg_quantity']:.2f}\")\n",
    "    print(f\"  Avg Price: ${metrics['avg_price']:.2f}\")\n",
    "    print(f\"  Total Quantity Sold: {metrics['total_quantity']:,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Run cleaning\n",
    "df_clean = comprehensive_cleaning(df_raw)\n",
    "\n",
    "# Show cleaned data sample\n",
    "print(\"\\nâœ… CLEANED DATA SAMPLE:\")\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "478da2a7-9128-4674-b187-92b82feaebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ›’ PREPARING TRANSACTION DATA (FIXED)\n",
      "----------------------------------------\n",
      "Using collect_set() to ensure unique items in each transaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 36,722 transaction baskets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS: All transactions verified to have unique items\n",
      "\n",
      "ðŸ” FINAL VERIFICATION:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample transaction has 8 items, all unique: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 5: Transaction Preparation\n",
    "def prepare_transactions_fixed(df_clean):\n",
    "    \"\"\"Fixed transaction preparation ensuring unique items\"\"\"\n",
    "    print(\"\\nðŸ›’ PREPARING TRANSACTION DATA (FIXED)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Use collect_set to automatically get unique items\n",
    "    print(\"Using collect_set() to ensure unique items in each transaction...\")\n",
    "    transactions_df = df_clean.groupBy(\"InvoiceNo\") \\\n",
    "        .agg(collect_set(\"StockCode\").alias(\"items\")) \\\n",
    "        .filter(size(col(\"items\")) > 0)\n",
    "    \n",
    "    # Double-check for duplicates\n",
    "    transactions_df = transactions_df.withColumn(\"items\", array_distinct(col(\"items\")))\n",
    "    transactions_df = transactions_df.withColumn(\"basket_size\", size(col(\"items\")))\n",
    "    \n",
    "    transaction_count = transactions_df.count()\n",
    "    print(f\"âœ“ Created {transaction_count:,} transaction baskets\")\n",
    "    \n",
    "    # Final verification\n",
    "    final_check = transactions_df.select(\n",
    "        sum(when(size(col(\"items\")) != size(array_distinct(col(\"items\"))), 1).otherwise(0)).alias(\"remaining_duplicates\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    if final_check['remaining_duplicates'] == 0:\n",
    "        print(\"âœ… SUCCESS: All transactions verified to have unique items\")\n",
    "    else:\n",
    "        print(f\"âŒ CRITICAL: {final_check['remaining_duplicates']} transactions still have duplicates\")\n",
    "        # Force remove duplicates with UDF as last resort\n",
    "        from pyspark.sql.functions import udf\n",
    "        from pyspark.sql.types import ArrayType, StringType\n",
    "        \n",
    "        @udf(ArrayType(StringType()))\n",
    "        def force_unique(items):\n",
    "            return list(set(items))\n",
    "        \n",
    "        transactions_df = transactions_df.withColumn(\"items\", force_unique(col(\"items\")))\n",
    "        transactions_df = transactions_df.withColumn(\"basket_size\", size(col(\"items\")))\n",
    "        print(\"âœ“ Applied force_unique UDF as last resort\")\n",
    "    \n",
    "    return transactions_df\n",
    "\n",
    "# Prepare transactions with the fixed method\n",
    "transactions_df = prepare_transactions_fixed(df_clean)\n",
    "\n",
    "# Show final verification\n",
    "print(\"\\nðŸ” FINAL VERIFICATION:\")\n",
    "sample = transactions_df.limit(1).collect()[0]\n",
    "items = sample['items']\n",
    "is_unique = len(items) == len(set(items))\n",
    "print(f\"Sample transaction has {len(items)} items, all unique: {is_unique}\")\n",
    "if not is_unique:\n",
    "    print(f\"Duplicates found: {[item for item in items if items.count(item) > 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3a2f56-da11-443d-9a60-fb2cd38ac74a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Cell 6: Run FP-Growth\n",
    "# def run_fp_growth_analysis(transactions_df, min_support=0.005, min_confidence=0.4):\n",
    "#     \"\"\"Run FP-Growth algorithm and generate association rules\"\"\"\n",
    "#     print(f\"\\nðŸ” RUNNING FP-GROWTH ANALYSIS\")\n",
    "#     print(\"-\" * 40)\n",
    "#     print(f\"Minimum Support: {min_support}\")\n",
    "#     print(f\"Minimum Confidence: {min_confidence}\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Initialize and run FP-Growth\n",
    "#     fp_growth = FPGrowth(\n",
    "#         itemsCol=\"items\", \n",
    "#         minSupport=min_support, \n",
    "#         minConfidence=min_confidence,\n",
    "#         numPartitions=8\n",
    "#     )\n",
    "    \n",
    "#     print(\"Training FP-Growth model...\")\n",
    "#     model = fp_growth.fit(transactions_df)\n",
    "    \n",
    "#     # Get results\n",
    "#     frequent_itemsets = model.freqItemsets\n",
    "#     association_rules = model.associationRules\n",
    "    \n",
    "#     processing_time = time.time() - start_time\n",
    "    \n",
    "#     print(f\"âœ“ FP-Growth completed in {processing_time:.2f} seconds\")\n",
    "#     print(f\"âœ“ Frequent itemsets: {frequent_itemsets.count():,}\")\n",
    "#     print(f\"âœ“ Association rules: {association_rules.count():,}\")\n",
    "    \n",
    "#     return model, frequent_itemsets, association_rules\n",
    "\n",
    "# # Run FP-Growth\n",
    "# model, frequent_itemsets, association_rules = run_fp_growth_analysis(transactions_df)\n",
    "\n",
    "# # Show sample results\n",
    "# print(\"\\nðŸ” SAMPLE RESULTS:\")\n",
    "# print(\"Frequent Itemsets sample:\")\n",
    "# frequent_itemsets.show(5)\n",
    "# print(\"Association Rules sample:\")\n",
    "# association_rules.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba60ad8-b480-4545-93d3-8d2af4edfc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” RUNNING OPTIMIZED FP-GROWTH ANALYSIS\n",
      "----------------------------------------\n",
      "Minimum Support: 0.01 (INCREASED)\n",
      "Minimum Confidence: 0.5\n",
      "Training FP-Growth model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FP-Growth completed in 22.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Frequent itemsets: 853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:============================>                            (5 + 2) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Association rules: 241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 6: Run FP-Growth (Optimized Version)\n",
    "def run_fp_growth_optimized(transactions_df, min_support=0.01, min_confidence=0.5):\n",
    "    \"\"\"Run FP-Growth with optimized parameters to avoid StackOverflow\"\"\"\n",
    "    print(f\"\\nðŸ” RUNNING OPTIMIZED FP-GROWTH ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Minimum Support: {min_support} (INCREASED)\")\n",
    "    print(f\"Minimum Confidence: {min_confidence}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize and run FP-Growth with optimized settings\n",
    "    fp_growth = FPGrowth(\n",
    "        itemsCol=\"items\", \n",
    "        minSupport=min_support,  # Higher support to reduce complexity\n",
    "        minConfidence=min_confidence,\n",
    "        numPartitions=10,  # More partitions for better distribution\n",
    "    )\n",
    "    \n",
    "    print(\"Training FP-Growth model...\")\n",
    "    try:\n",
    "        model = fp_growth.fit(transactions_df)\n",
    "        \n",
    "        # Get results with safe collection\n",
    "        frequent_itemsets = model.freqItemsets.cache()\n",
    "        association_rules = model.associationRules.cache()\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"âœ“ FP-Growth completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        # Safe counting with try-catch\n",
    "        try:\n",
    "            itemset_count = frequent_itemsets.count()\n",
    "            print(f\"âœ“ Frequent itemsets: {itemset_count:,}\")\n",
    "        except:\n",
    "            print(\"âš  Could not count frequent itemsets\")\n",
    "            itemset_count = 0\n",
    "            \n",
    "        try:\n",
    "            rules_count = association_rules.count()\n",
    "            print(f\"âœ“ Association rules: {rules_count:,}\")\n",
    "        except:\n",
    "            print(\"âš  Could not count association rules\")\n",
    "            rules_count = 0\n",
    "        \n",
    "        return model, frequent_itemsets, association_rules\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ FP-Growth failed: {str(e)}\")\n",
    "        print(\"Trying with even higher support threshold...\")\n",
    "        return None, None, None\n",
    "\n",
    "# Run optimized FP-Growth\n",
    "model, frequent_itemsets, association_rules = run_fp_growth_optimized(transactions_df)\n",
    "\n",
    "if model is None:\n",
    "    print(\"\\nðŸ”„ TRYING WITH HIGHER SUPPORT THRESHOLD\")\n",
    "    model, frequent_itemsets, association_rules = run_fp_growth_optimized(\n",
    "        transactions_df, min_support=0.02, min_confidence=0.6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4c2047-a7d7-4b61-8469-938c8e8feb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving DF_CLEAN\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - cleaned_data/ (Parquet + CSV)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "def save_results_df_clean(df_clean):\n",
    "    \"\"\"Save all df_clean\"\"\"\n",
    "    print(\"\\nSaving DF_CLEAN\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"csv_results\", exist_ok=True)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df_clean.write.mode(\"overwrite\").parquet(\"csv_results/cleaned_data\")\n",
    "    df_clean.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/cleaned_data_csv\")\n",
    "    \n",
    "    print(\"  - cleaned_data/ (Parquet + CSV)\")\n",
    "\n",
    "\n",
    "# Save results\n",
    "save_results_df_clean(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd847084-8c31-49e1-a993-674c6c4e7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ SAVING RESULTS\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All results saved to 'csv_results' folder:\n",
      "  - cleaned_data/ (Parquet + CSV)\n",
      "  - frequent_itemsets/ (Parquet + CSV)\n",
      "  - association_rules/ (Parquet + CSV)\n",
      "  - transactions/ (Parquet)\n",
      "  - sample_data/ (CSV)\n",
      "\n",
      "ðŸ“Š Sample of converted frequent itemsets (CSV format):\n",
      "+-----+----+\n",
      "|items|freq|\n",
      "+-----+----+\n",
      "|21155|724 |\n",
      "|21535|956 |\n",
      "|22567|431 |\n",
      "|22602|405 |\n",
      "|22666|1277|\n",
      "+-----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ðŸ“Š Sample of converted association rules (CSV format):\n",
      "+------------+----------+------------------+------------------+--------------------+\n",
      "|antecedent  |consequent|confidence        |lift              |support             |\n",
      "+------------+----------+------------------+------------------+--------------------+\n",
      "|22749       |22750     |0.6103059581320451|37.540461297361745|0.010320788628070366|\n",
      "|22554       |22551     |0.5059252506836828|16.07144208962474 |0.015113555906541037|\n",
      "|22383, 20725|22384     |0.5543032786885246|11.03258807588076 |0.014732313054844508|\n",
      "|22383, 20725|20728     |0.5092213114754098|10.406023928770173|0.01353412123522684 |\n",
      "|22383, 20725|20727     |0.5153688524590164|9.476902854281423 |0.013697511028811067|\n",
      "+------------+----------+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def save_results(df_clean, frequent_itemsets, association_rules, transactions_df):\n",
    "    \"\"\"Save all results to files\"\"\"\n",
    "    print(\"\\nðŸ’¾ SAVING RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"csv_results\", exist_ok=True)\n",
    "    \n",
    "    # Save cleaned data (this should work fine)\n",
    "    df_clean.write.mode(\"overwrite\").parquet(\"csv_results/cleaned_data\")\n",
    "    df_clean.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/cleaned_data_csv\")\n",
    "    \n",
    "    # Save frequent itemsets - FIXED: Convert arrays to strings for CSV\n",
    "    frequent_itemsets.write.mode(\"overwrite\").parquet(\"csv_results/frequent_itemsets\")\n",
    "    \n",
    "    # Convert array columns to string for CSV export\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    \n",
    "    # Convert frequent_itemsets arrays to comma-separated strings\n",
    "    fi_csv = frequent_itemsets.withColumn(\"items\", concat_ws(\", \", col(\"items\")))\n",
    "    fi_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/frequent_itemsets_csv\")\n",
    "    \n",
    "    # Save association rules - FIXED: Convert arrays to strings for CSV\n",
    "    association_rules.write.mode(\"overwrite\").parquet(\"csv_results/association_rules\")\n",
    "    \n",
    "    # Convert association_rules arrays to comma-separated strings\n",
    "    ar_csv = association_rules.withColumn(\"antecedent\", concat_ws(\", \", col(\"antecedent\"))) \\\n",
    "                             .withColumn(\"consequent\", concat_ws(\", \", col(\"consequent\")))\n",
    "    ar_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/association_rules_csv\")\n",
    "    \n",
    "    # Save transaction data\n",
    "    transactions_df.write.mode(\"overwrite\").parquet(\"csv_results/transactions\")\n",
    "    \n",
    "    # Save sample data for quick inspection\n",
    "    df_clean.limit(10000).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/sample_data\")\n",
    "    \n",
    "    print(\"âœ“ All results saved to 'csv_results' folder:\")\n",
    "    print(\"  - cleaned_data/ (Parquet + CSV)\")\n",
    "    print(\"  - frequent_itemsets/ (Parquet + CSV)\")\n",
    "    print(\"  - association_rules/ (Parquet + CSV)\")\n",
    "    print(\"  - transactions/ (Parquet)\")\n",
    "    print(\"  - sample_data/ (CSV)\")\n",
    "    \n",
    "    # Show sample of converted data\n",
    "    print(\"\\nðŸ“Š Sample of converted frequent itemsets (CSV format):\")\n",
    "    fi_csv.show(5, truncate=False)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Sample of converted association rules (CSV format):\")\n",
    "    ar_csv.show(5, truncate=False)\n",
    "\n",
    "# Save results with fixed function\n",
    "save_results(df_clean, frequent_itemsets, association_rules, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5017b-3813-46c9-9c28-eec679f26a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analyze Results\n",
    "def analyze_results(frequent_itemsets, association_rules, transactions_count):\n",
    "    \"\"\"Analyze and display FP-Growth results\"\"\"\n",
    "    print(\"\\nðŸ“Š ANALYZING RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    freq_itemsets_pd = frequent_itemsets.orderBy(desc(\"freq\")).limit(20).toPandas()\n",
    "    rules_pd = association_rules.orderBy(desc(\"confidence\")).limit(20).toPandas()\n",
    "    \n",
    "    # Calculate support for itemsets\n",
    "    freq_itemsets_pd['support'] = freq_itemsets_pd['freq'] / transactions_count\n",
    "    \n",
    "    print(\"ðŸ† TOP 10 FREQUENT ITEMSETS:\")\n",
    "    for i, row in freq_itemsets_pd.head(10).iterrows():\n",
    "        items_str = str(row['items'])[:80] + \"...\" if len(str(row['items'])) > 80 else str(row['items'])\n",
    "        print(f\"  {i+1:2d}. Support: {row['support']:.4f} - Items: {items_str}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ TOP 10 ASSOCIATION RULES:\")\n",
    "    for i, row in rules_pd.head(10).iterrows():\n",
    "        support = row['freq'] / transactions_count\n",
    "        antecedent_str = str(row['antecedent'])[:50] + \"...\" if len(str(row['antecedent'])) > 50 else str(row['antecedent'])\n",
    "        consequent_str = str(row['consequent'])[:50] + \"...\" if len(str(row['consequent'])) > 50 else str(row['consequent'])\n",
    "        print(f\"  {i+1:2d}. {antecedent_str} â†’ {consequent_str}\")\n",
    "        print(f\"      Confidence: {row['confidence']:.3f}, Support: {support:.4f}\")\n",
    "    \n",
    "    # Rule quality analysis\n",
    "    if len(rules_pd) > 0:\n",
    "        print(f\"\\nðŸ“ˆ RULE QUALITY SUMMARY:\")\n",
    "        print(f\"  Total rules: {len(rules_pd):,}\")\n",
    "        print(f\"  Avg confidence: {rules_pd['confidence'].mean():.3f}\")\n",
    "        print(f\"  Max confidence: {rules_pd['confidence'].max():.3f}\")\n",
    "        print(f\"  High confidence rules (>0.7): {len(rules_pd[rules_pd['confidence'] > 0.7])}\")\n",
    "        print(f\"  Medium confidence rules (>0.5): {len(rules_pd[rules_pd['confidence'] > 0.5])}\")\n",
    "        print(f\"  Avg rule length: {rules_pd['antecedent'].apply(len).mean() + rules_pd['consequent'].apply(len).mean():.1f} items\")\n",
    "    \n",
    "    return freq_itemsets_pd, rules_pd\n",
    "\n",
    "# Analyze results\n",
    "transaction_count = transactions_df.count()\n",
    "freq_itemsets_pd, rules_pd = analyze_results(frequent_itemsets, association_rules, transaction_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592d1f5-9c96-4670-9fe0-96324ffa88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create Visualizations\n",
    "def create_visualizations(freq_itemsets_pd, rules_pd, df_clean, transactions_df):\n",
    "    \"\"\"Create visualizations for the analysis\"\"\"\n",
    "    print(\"\\nðŸ“ˆ CREATING VISUALIZATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Set up plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Market Basket Analysis - Online Retail II', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Top frequent itemsets by support\n",
    "    if len(freq_itemsets_pd) > 0:\n",
    "        top_itemsets = freq_itemsets_pd.head(8)\n",
    "        # Create shortened labels\n",
    "        labels = []\n",
    "        for items in top_itemsets['items']:\n",
    "            if len(items) == 1:\n",
    "                labels.append(f\"Single: {items[0]}\")\n",
    "            else:\n",
    "                labels.append(f\"Combo: {len(items)} items\")\n",
    "        \n",
    "        axes[0, 0].barh(range(len(top_itemsets)), top_itemsets['support'])\n",
    "        axes[0, 0].set_yticks(range(len(top_itemsets)))\n",
    "        axes[0, 0].set_yticklabels(labels)\n",
    "        axes[0, 0].set_title('Top 8 Frequent Itemsets by Support', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Support')\n",
    "        axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Confidence distribution of rules\n",
    "    if len(rules_pd) > 0:\n",
    "        axes[0, 1].hist(rules_pd['confidence'], bins=20, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "        axes[0, 1].set_title('Distribution of Rule Confidence', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Confidence')\n",
    "        axes[0, 1].set_ylabel('Number of Rules')\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Top rules by confidence\n",
    "    if len(rules_pd) > 0:\n",
    "        top_rules = rules_pd.head(6)\n",
    "        y_pos = np.arange(len(top_rules))\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(top_rules)))\n",
    "        \n",
    "        bars = axes[1, 0].barh(y_pos, top_rules['confidence'], color=colors)\n",
    "        axes[1, 0].set_yticks(y_pos)\n",
    "        axes[1, 0].set_yticklabels([f\"Rule {i+1}\" for i in range(len(top_rules))])\n",
    "        axes[1, 0].set_title('Top 6 Rules by Confidence', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Confidence')\n",
    "        axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add confidence values on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            axes[1, 0].text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                           f'{width:.3f}', ha='left', va='center')\n",
    "    \n",
    "    # Plot 4: Basket size distribution\n",
    "    basket_sizes_pd = transactions_df.select(\"basket_size\").toPandas()\n",
    "    axes[1, 1].hist(basket_sizes_pd['basket_size'], bins=50, alpha=0.7, \n",
    "                   edgecolor='black', color='lightgreen')\n",
    "    axes[1, 1].set_title('Distribution of Basket Sizes', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Items per Basket')\n",
    "    axes[1, 1].set_ylabel('Frequency (log scale)')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional visualization: Support vs Confidence scatter plot\n",
    "    if len(rules_pd) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(rules_pd['freq']/transaction_count, rules_pd['confidence'], \n",
    "                            alpha=0.6, c=rules_pd['confidence'], cmap='viridis')\n",
    "        plt.colorbar(scatter, label='Confidence')\n",
    "        plt.xlabel('Support')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.title('Support vs Confidence for Association Rules', fontweight='bold')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"âœ“ Visualizations created successfully\")\n",
    "\n",
    "# Create visualizations\n",
    "create_visualizations(freq_itemsets_pd, rules_pd, df_clean, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284dc52-b62f-462b-8ba5-67746b8127bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "def save_results(df_clean, frequent_itemsets, association_rules, transactions_df):\n",
    "    \"\"\"Save all results to files\"\"\"\n",
    "    print(\"\\nðŸ’¾ SAVING RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"csv_results\", exist_ok=True)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df_clean.write.mode(\"overwrite\").parquet(\"csv_results/cleaned_data\")\n",
    "    df_clean.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/cleaned_data_csv\")\n",
    "    \n",
    "    # Save frequent itemsets\n",
    "    frequent_itemsets.write.mode(\"overwrite\").parquet(\"csv_results/frequent_itemsets\")\n",
    "    frequent_itemsets.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/frequent_itemsets_csv\")\n",
    "    \n",
    "    # Save association rules\n",
    "    association_rules.write.mode(\"overwrite\").parquet(\"csv_results/association_rules\")\n",
    "    association_rules.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/association_rules_csv\")\n",
    "    \n",
    "    # Save transaction data\n",
    "    transactions_df.write.mode(\"overwrite\").parquet(\"csv_results/transactions\")\n",
    "    \n",
    "    # Save sample data for quick inspection\n",
    "    df_clean.limit(10000).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"csv_results/sample_data\")\n",
    "    \n",
    "    print(\"âœ“ All results saved to 'csv_results' folder:\")\n",
    "    print(\"  - cleaned_data/ (Parquet + CSV)\")\n",
    "    print(\"  - frequent_itemsets/ (Parquet + CSV)\")\n",
    "    print(\"  - association_rules/ (Parquet + CSV)\")\n",
    "    print(\"  - transactions/ (Parquet)\")\n",
    "    print(\"  - sample_data/ (CSV)\")\n",
    "\n",
    "# Save results\n",
    "save_results(df_clean, frequent_itemsets, association_rules, transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaa2e2-0d7d-41c9-ba0f-22dbd35841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Generate Business Insights\n",
    "def generate_business_insights(rules_pd, freq_itemsets_pd, top_n=10):\n",
    "    \"\"\"Generate actionable business insights\"\"\"\n",
    "    print(\"\\nðŸ’¡ BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"ðŸŽ¯ TOP CROSS-SELLING OPPORTUNITIES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, row in rules_pd.head(top_n).iterrows():\n",
    "        print(f\"\\n{i+1}. WHEN customers buy: {row['antecedent']}\")\n",
    "        print(f\"   THEY ALSO buy: {row['consequent']}\")\n",
    "        print(f\"   Confidence: {row['confidence']:.1%}\")\n",
    "        \n",
    "        # Business recommendation based on confidence\n",
    "        if row['confidence'] >= 0.7:\n",
    "            recommendation = \"ðŸ’ª STRONG BUNDLE - Create product bundles and recommend together\"\n",
    "        elif row['confidence'] >= 0.5:\n",
    "            recommendation = \"ðŸ‘ MODERATE OPPORTUNITY - Cross-sell promotions and suggestions\"\n",
    "        else:\n",
    "            recommendation = \"ðŸ‘€ WEAK ASSOCIATION - Monitor and test with discounts\"\n",
    "        \n",
    "        print(f\"   ðŸ’¡ ACTION: {recommendation}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ POPULAR PRODUCT COMBINATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, row in freq_itemsets_pd.head(5).iterrows():\n",
    "        if len(row['items']) == 1:\n",
    "            print(f\"  {i+1}. Single popular item: {row['items']} (Support: {row['support']:.2%})\")\n",
    "        else:\n",
    "            print(f\"  {i+1}. Popular combination: {row['items']} (Support: {row['support']:.2%})\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nðŸ“ˆ PROJECT SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Total association rules found: {len(rules_pd):,}\")\n",
    "    print(f\"  High-confidence rules (â‰¥70%): {len(rules_pd[rules_pd['confidence'] >= 0.7])}\")\n",
    "    print(f\"  Medium-confidence rules (â‰¥50%): {len(rules_pd[rules_pd['confidence'] >= 0.5])}\")\n",
    "    print(f\"  Average rule confidence: {rules_pd['confidence'].mean():.1%}\")\n",
    "\n",
    "# Generate insights\n",
    "generate_business_insights(rules_pd, freq_itemsets_pd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:csgyc-6513-fall]",
   "language": "python",
   "name": "conda-env-csgyc-6513-fall-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
