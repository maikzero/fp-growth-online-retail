{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76dbd17-f96c-433a-a65e-59ebd89ffe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARKET BASKET ANALYSIS - PHASES 1 & 2 (Local Download Version)\n",
    "# This version downloads the dataset locally and tests each step\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97703ded-def4-4cea-aaab-551326f424b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ATTEMPTING TO LOAD CSV ===\n",
      "Trying encoding: latin-1\n",
      "‚úÖ Successfully loaded CSV with latin-1 encoding\n",
      "Trying encoding: ISO-8859-1\n",
      "‚úÖ Successfully loaded CSV with ISO-8859-1 encoding\n",
      "Trying encoding: cp1252\n",
      "‚úÖ Successfully loaded CSV with cp1252 encoding\n",
      "Trying encoding: utf-16\n",
      "‚ùå Error with utf-16: UTF-16 stream does not start with BOM\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Method 1: Try loading CSV with common encodings\n",
    "def load_csv_with_encoding(file_path):\n",
    "    encodings = ['latin-1', 'ISO-8859-1', 'cp1252', 'utf-16']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Trying encoding: {encoding}\")\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"‚úÖ Successfully loaded CSV with {encoding} encoding\")\n",
    "            # return df\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"‚ùå Failed with {encoding}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {encoding}: {e}\")\n",
    "    return df\n",
    "    return None\n",
    "\n",
    "# Try to load CSV\n",
    "print(\"=== ATTEMPTING TO LOAD CSV ===\")\n",
    "csv_file = 'OnlineRetail.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df_csv = load_csv_with_encoding(csv_file)\n",
    "else:\n",
    "    print(f\"‚ùå CSV file not found: {csv_file}\")\n",
    "    df_csv = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af118aee-a506-4c5f-8fec-e732a5197299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset():\n",
    "    \"\"\"Look for the dataset in various formats and locations\"\"\"\n",
    "    print(\"=== SEARCHING FOR DATASET ===\")\n",
    "    \n",
    "    possible_files = [\n",
    "        'OnlineRetail.csv',\n",
    "        'OnlineRetail.xlsx',  # Original from UCI/Kaggle\n",
    "        'Online Retail.xlsx',  # Original from UCI/Kaggle\n",
    "        'online_retail.csv',  # Possible CSV version\n",
    "        \n",
    "        'onlineretail.csv',\n",
    "        '/kaggle/input/onlineretail/OnlineRetail.xlsx',  # Kaggle path\n",
    "    ]\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"‚úì Found dataset: {file_path}\")\n",
    "            return file_path\n",
    "    \n",
    "    print(\"‚ùå No dataset file found locally\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "    \"\"\"Download the dataset from UCI repository\"\"\"\n",
    "    print(\"=== DOWNLOADING DATASET ===\")\n",
    "    \n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"\n",
    "    local_filename = \"Online Retail.xlsx\"\n",
    "    # local_filename = \"OnlineRetail.csv\"\n",
    "    \n",
    "    # Check if file already exists\n",
    "    if os.path.exists(local_filename):\n",
    "        print(f\"‚úì Dataset already exists locally: {local_filename}\")\n",
    "        return local_filename\n",
    "    \n",
    "    print(f\"Downloading dataset from: {url}\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Get file size\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(local_filename, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        progress = (downloaded / total_size) * 100\n",
    "                        print(f\"Progress: {progress:.1f}%\", end='\\r')\n",
    "        \n",
    "        print(f\"\\n‚úì Dataset downloaded successfully: {local_filename}\")\n",
    "        print(f\"‚úì File size: {os.path.getsize(local_filename) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return local_filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset based on file format\"\"\"\n",
    "    print(f\"\\n=== LOADING DATASET: {file_path} ===\")\n",
    "    \n",
    "    try:\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            print(\"Loading Excel file...\")\n",
    "            df = pd.read_excel(file_path)\n",
    "            print(\"‚úì Successfully loaded Excel file\")\n",
    "        elif file_path.endswith('.csv'):\n",
    "            print(\"Loading CSV file...\")\n",
    "            df = pd.read_csv(file_path, encoding='latin-1')\n",
    "            print(\"‚úì Successfully loaded CSV file\")\n",
    "        else:\n",
    "            print(\"‚ùå Unsupported file format\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úì Dataset shape: {df.shape}\")\n",
    "        print(f\"‚úì Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_data_loading(file_path):\n",
    "    \"\"\"Test loading the dataset\"\"\"\n",
    "    print(\"\\n=== TESTING DATA LOADING ===\")\n",
    "    \n",
    "    try:\n",
    "        # Test reading the file\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"‚úì Successfully loaded dataset\")\n",
    "        print(f\"‚úì Shape: {df.shape}\")\n",
    "        print(f\"‚úì Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def phase1_exploration(df):\n",
    "    \"\"\"Perform Phase 1: Data Exploration\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1: PROJECT SETUP & DATA UNDERSTANDING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1.1 Basic Information\n",
    "    print(\"\\n1.1 BASIC DATASET INFORMATION\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Number of records: {df.shape[0]:,}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    \n",
    "    # 1.2 Display sample data\n",
    "    print(\"\\n1.2 SAMPLE DATA\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"First 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # 1.3 Data types and info\n",
    "    print(\"\\n1.3 DATA TYPES AND INFO\")\n",
    "    print(\"-\" * 40)\n",
    "    print(df.info())\n",
    "    \n",
    "    # 1.4 Missing values analysis\n",
    "    print(\"\\n1.4 MISSING VALUES ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing Percentage': missing_percent\n",
    "    })\n",
    "    print(missing_info)\n",
    "    \n",
    "    # 1.5 Data Quality Issues\n",
    "    print(\"\\n1.5 DATA QUALITY ISSUES\")\n",
    "    print(\"-\" * 40)\n",
    "    negative_quantity = (df['Quantity'] <= 0).sum()\n",
    "    negative_price = (df['UnitPrice'] <= 0).sum()\n",
    "    cancelled_invoices = df['InvoiceNo'].astype(str).str.startswith('C').sum()\n",
    "    \n",
    "    print(f\"Records with Quantity <= 0: {negative_quantity:,} ({negative_quantity/len(df)*100:.2f}%)\")\n",
    "    print(f\"Records with UnitPrice <= 0: {negative_price:,} ({negative_price/len(df)*100:.2f}%)\")\n",
    "    print(f\"Cancelled invoices (starting with 'C'): {cancelled_invoices:,} ({cancelled_invoices/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # 1.6 Key Business Metrics\n",
    "    print(\"\\n1.6 KEY BUSINESS METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Convert InvoiceDate to datetime\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    \n",
    "    total_transactions = df['InvoiceNo'].nunique()\n",
    "    total_products = df['StockCode'].nunique()\n",
    "    total_customers = df['CustomerID'].nunique()\n",
    "    date_range = f\"{df['InvoiceDate'].min().strftime('%Y-%m-%d')} to {df['InvoiceDate'].max().strftime('%Y-%m-%d')}\"\n",
    "    \n",
    "    print(f\"Total transactions: {total_transactions:,}\")\n",
    "    print(f\"Unique products: {total_products:,}\")\n",
    "    print(f\"Unique customers: {total_customers:,}\")\n",
    "    print(f\"Time period: {date_range}\")\n",
    "    \n",
    "    # Countries distribution\n",
    "    country_count = df['Country'].nunique()\n",
    "    top_countries = df['Country'].value_counts().head(3)\n",
    "    print(f\"Number of countries: {country_count}\")\n",
    "    print(\"Top 3 countries:\")\n",
    "    print(top_countries)\n",
    "    \n",
    "    return {\n",
    "        'original_shape': df.shape,\n",
    "        'missing_info': missing_info,\n",
    "        'negative_quantity': negative_quantity,\n",
    "        'negative_price': negative_price,\n",
    "        'cancelled_invoices': cancelled_invoices,\n",
    "        'total_transactions': total_transactions,\n",
    "        'total_products': total_products,\n",
    "        'total_customers': total_customers\n",
    "    }\n",
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"Create initial visualizations\"\"\"\n",
    "    print(\"\\n1.7 CREATING VISUALIZATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create a simple visualization folder\n",
    "    if not os.path.exists('visualizations'):\n",
    "        os.makedirs('visualizations')\n",
    "    \n",
    "    try:\n",
    "        # Plot 1: Quantity distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(df[df['Quantity'] < 100]['Quantity'], bins=50, edgecolor='black', alpha=0.7)\n",
    "        plt.title('Distribution of Quantity (Quantities < 100)')\n",
    "        plt.xlabel('Quantity')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig('visualizations/quantity_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Created quantity distribution plot\")\n",
    "        \n",
    "        # Plot 2: UnitPrice distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(df[df['UnitPrice'] < 50]['UnitPrice'], bins=50, edgecolor='black', alpha=0.7)\n",
    "        plt.title('Distribution of UnitPrice (Prices < $50)')\n",
    "        plt.xlabel('UnitPrice ($)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig('visualizations/price_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Created price distribution plot\")\n",
    "        \n",
    "        # Plot 3: Top products\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "        plt.barh(range(len(top_products)), top_products.values)\n",
    "        plt.yticks(range(len(top_products)), [desc[:40] + '...' if len(desc) > 40 else desc for desc in top_products.index])\n",
    "        plt.title('Top 10 Products by Quantity Sold')\n",
    "        plt.xlabel('Total Quantity Sold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/top_products.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Created top products plot\")\n",
    "        \n",
    "        # Plot 4: Monthly transactions\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        monthly_tx = df.set_index('InvoiceDate').resample('M')['InvoiceNo'].nunique()\n",
    "        plt.plot(monthly_tx.index, monthly_tx.values, marker='o')\n",
    "        plt.title('Monthly Transactions Over Time')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/monthly_transactions.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Created monthly transactions plot\")\n",
    "        \n",
    "        plt.close('all')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not create all visualizations: {e}\")\n",
    "\n",
    "def phase2_cleaning(df, phase1_results):\n",
    "    \"\"\"Perform Phase 2: Data Cleaning\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2: DATA PREPROCESSING & CLEANING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    original_size = len(df)\n",
    "    print(f\"Starting with {original_size:,} records\")\n",
    "    \n",
    "    # 2.1 Apply cleaning steps\n",
    "    print(\"\\n2.1 APPLYING CLEANING STEPS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Step 1: Remove cancellations\n",
    "    df_clean = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "    removed_cancelled = original_size - len(df_clean)\n",
    "    print(f\"‚úì Removed cancellations: {removed_cancelled:,} records\")\n",
    "    \n",
    "    # Step 2: Remove invalid quantities\n",
    "    df_clean = df_clean[df_clean['Quantity'] > 0]\n",
    "    removed_quantity = original_size - len(df_clean) - removed_cancelled\n",
    "    print(f\"‚úì Removed invalid quantities: {removed_quantity:,} records\")\n",
    "    \n",
    "    # Step 3: Remove invalid prices\n",
    "    df_clean = df_clean[df_clean['UnitPrice'] > 0]\n",
    "    removed_price = original_size - len(df_clean) - removed_cancelled - removed_quantity\n",
    "    print(f\"‚úì Removed invalid prices: {removed_price:,} records\")\n",
    "    \n",
    "    # Step 4: Remove missing CustomerID\n",
    "    df_clean = df_clean[df_clean['CustomerID'].notnull()]\n",
    "    removed_customerid = phase1_results['missing_info'].loc['CustomerID', 'Missing Count']\n",
    "    print(f\"‚úì Removed missing CustomerID: {removed_customerid:,} records\")\n",
    "    \n",
    "    # Step 5: Remove missing Description\n",
    "    df_clean = df_clean[df_clean['Description'].notnull()]\n",
    "    removed_description = phase1_results['missing_info'].loc['Description', 'Missing Count']\n",
    "    print(f\"‚úì Removed missing Description: {removed_description:,} records\")\n",
    "    \n",
    "    # 2.2 Data Standardization\n",
    "    print(\"\\n2.2 DATA STANDARDIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_clean['Description'] = df_clean['Description'].str.upper().str.strip()\n",
    "    print(\"‚úì Standardized product descriptions\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    duplicates = df_clean.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        print(f\"‚úì Removed duplicates: {duplicates:,} records\")\n",
    "    \n",
    "    # 2.3 Post-cleaning analysis\n",
    "    print(\"\\n2.3 POST-CLEANING ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    cleaned_size = len(df_clean)\n",
    "    total_removed = original_size - cleaned_size\n",
    "    retention_rate = (cleaned_size / original_size) * 100\n",
    "    \n",
    "    print(f\"Original dataset: {original_size:,} records\")\n",
    "    print(f\"Cleaned dataset: {cleaned_size:,} records\")\n",
    "    print(f\"Records removed: {total_removed:,} records\")\n",
    "    print(f\"Retention rate: {retention_rate:.2f}%\")\n",
    "    \n",
    "    # Key metrics after cleaning\n",
    "    print(f\"\\nKey metrics after cleaning:\")\n",
    "    print(f\"Transactions: {df_clean['InvoiceNo'].nunique():,}\")\n",
    "    print(f\"Products: {df_clean['StockCode'].nunique():,}\")\n",
    "    print(f\"Customers: {df_clean['CustomerID'].nunique():,}\")\n",
    "    \n",
    "    # 2.4 Prepare transaction data\n",
    "    print(\"\\n2.4 PREPARING TRANSACTION DATA\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create transaction baskets\n",
    "    basket_data = df_clean.groupby('InvoiceNo')['StockCode'].apply(list).reset_index()\n",
    "    print(f\"‚úì Created {len(basket_data):,} transaction baskets\")\n",
    "    \n",
    "    # Show sample baskets\n",
    "    print(\"\\nSample transaction baskets:\")\n",
    "    for i in range(min(3, len(basket_data))):\n",
    "        basket = basket_data.iloc[i]\n",
    "        print(f\"  Invoice {basket['InvoiceNo']}: {len(basket['StockCode'])} items\")\n",
    "    \n",
    "    # 2.5 Save cleaned data\n",
    "    print(\"\\n2.5 SAVING CLEANED DATA\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_clean.to_csv('online_retail_cleaned.csv', index=False)\n",
    "    basket_data.to_csv('transaction_baskets.csv', index=False)\n",
    "    \n",
    "    print(\"‚úì Saved cleaned data: 'online_retail_cleaned.csv'\")\n",
    "    print(\"‚úì Saved transaction baskets: 'transaction_baskets.csv'\")\n",
    "    \n",
    "    return df_clean, basket_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b669adaf-b430-42df-a6be-04dc0909c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_report(phase1_results, df_clean):\n",
    "    \"\"\"Create a summary report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"üìä DATA QUALITY IMPROVEMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Original records: {phase1_results['original_shape'][0]:,}\")\n",
    "    print(f\"Cleaned records: {len(df_clean):,}\")\n",
    "    print(f\"Data quality improvement: {((phase1_results['original_shape'][0] - len(df_clean)) / phase1_results['original_shape'][0] * 100):.1f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ CLEANING ACTIONS COMPLETED:\")\n",
    "    print(\"  ‚Ä¢ Removed cancelled invoices\")\n",
    "    print(\"  ‚Ä¢ Removed invalid quantities and prices\") \n",
    "    print(\"  ‚Ä¢ Handled missing CustomerID and Description\")\n",
    "    print(\"  ‚Ä¢ Standardized product descriptions\")\n",
    "    print(\"  ‚Ä¢ Prepared transaction baskets for FP-Growth\")\n",
    "    \n",
    "    print(f\"\\nüéØ DATASET READY FOR ANALYSIS:\")\n",
    "    print(f\"  ‚Ä¢ {df_clean['InvoiceNo'].nunique():,} transactions\")\n",
    "    print(f\"  ‚Ä¢ {df_clean['StockCode'].nunique():,} products\") \n",
    "    print(f\"  ‚Ä¢ {df_clean['CustomerID'].nunique():,} customers\")\n",
    "    print(f\"  ‚Ä¢ Time period: {df_clean['InvoiceDate'].min().strftime('%Y-%m-%d')} to {df_clean['InvoiceDate'].max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    print(f\"\\nüìà NEXT STEPS:\")\n",
    "    print(f\"  ‚Ä¢ Proceed to Phase 3: FP-Growth Algorithm\")\n",
    "    print(f\"  ‚Ä¢ Generate association rules\")\n",
    "    print(f\"  ‚Ä¢ Implement temporal validation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47884b37-a9b7-499c-a529-be4f5a6b4a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MARKET BASKET ANALYSIS - LOCAL TEST ===\n",
      "This script will download the dataset and test Phases 1 & 2\n",
      "\n",
      "=== SEARCHING FOR DATASET ===\n",
      "‚úì Found dataset: OnlineRetail.csv\n",
      "\n",
      "=== LOADING DATASET: OnlineRetail.csv ===\n",
      "Loading CSV file...\n",
      "‚úì Successfully loaded CSV file\n",
      "‚úì Dataset shape: (541909, 8)\n",
      "‚úì Columns: ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']\n",
      "\n",
      "============================================================\n",
      "PHASE 1: PROJECT SETUP & DATA UNDERSTANDING\n",
      "============================================================\n",
      "\n",
      "1.1 BASIC DATASET INFORMATION\n",
      "----------------------------------------\n",
      "Dataset Shape: (541909, 8)\n",
      "Number of records: 541,909\n",
      "Number of columns: 8\n",
      "\n",
      "1.2 SAMPLE DATA\n",
      "----------------------------------------\n",
      "First 5 rows:\n",
      "  InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                  WHITE METAL LANTERN         6   \n",
      "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "\n",
      "1.3 DATA TYPES AND INFO\n",
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n",
      "None\n",
      "\n",
      "1.4 MISSING VALUES ANALYSIS\n",
      "----------------------------------------\n",
      "             Missing Count  Missing Percentage\n",
      "InvoiceNo                0            0.000000\n",
      "StockCode                0            0.000000\n",
      "Description           1454            0.268311\n",
      "Quantity                 0            0.000000\n",
      "InvoiceDate              0            0.000000\n",
      "UnitPrice                0            0.000000\n",
      "CustomerID          135080           24.926694\n",
      "Country                  0            0.000000\n",
      "\n",
      "1.5 DATA QUALITY ISSUES\n",
      "----------------------------------------\n",
      "Records with Quantity <= 0: 10,624 (1.96%)\n",
      "Records with UnitPrice <= 0: 2,517 (0.46%)\n",
      "Cancelled invoices (starting with 'C'): 9,288 (1.71%)\n",
      "\n",
      "1.6 KEY BUSINESS METRICS\n",
      "----------------------------------------\n",
      "Total transactions: 25,900\n",
      "Unique products: 4,070\n",
      "Unique customers: 4,372\n",
      "Time period: 2010-12-01 to 2011-12-09\n",
      "Number of countries: 38\n",
      "Top 3 countries:\n",
      "Country\n",
      "United Kingdom    495478\n",
      "Germany             9495\n",
      "France              8557\n",
      "Name: count, dtype: int64\n",
      "\n",
      "1.7 CREATING VISUALIZATIONS\n",
      "----------------------------------------\n",
      "‚úì Created quantity distribution plot\n",
      "‚úì Created price distribution plot\n",
      "‚úì Created top products plot\n",
      "‚úì Created monthly transactions plot\n",
      "\n",
      "============================================================\n",
      "PHASE 2: DATA PREPROCESSING & CLEANING\n",
      "============================================================\n",
      "Starting with 541,909 records\n",
      "\n",
      "2.1 APPLYING CLEANING STEPS\n",
      "----------------------------------------\n",
      "‚úì Removed cancellations: 9,288 records\n",
      "‚úì Removed invalid quantities: 1,336 records\n",
      "‚úì Removed invalid prices: 1,181 records\n",
      "‚úì Removed missing CustomerID: 135,080 records\n",
      "‚úì Removed missing Description: 1,454 records\n",
      "\n",
      "2.2 DATA STANDARDIZATION\n",
      "----------------------------------------\n",
      "‚úì Standardized product descriptions\n",
      "‚úì Removed duplicates: 5,192 records\n",
      "\n",
      "2.3 POST-CLEANING ANALYSIS\n",
      "----------------------------------------\n",
      "Original dataset: 541,909 records\n",
      "Cleaned dataset: 392,692 records\n",
      "Records removed: 149,217 records\n",
      "Retention rate: 72.46%\n",
      "\n",
      "Key metrics after cleaning:\n",
      "Transactions: 18,532\n",
      "Products: 3,665\n",
      "Customers: 4,338\n",
      "\n",
      "2.4 PREPARING TRANSACTION DATA\n",
      "----------------------------------------\n",
      "‚úì Created 18,532 transaction baskets\n",
      "\n",
      "Sample transaction baskets:\n",
      "  Invoice 536365: 7 items\n",
      "  Invoice 536366: 2 items\n",
      "  Invoice 536367: 12 items\n",
      "\n",
      "2.5 SAVING CLEANED DATA\n",
      "----------------------------------------\n",
      "‚úì Saved cleaned data: 'online_retail_cleaned.csv'\n",
      "‚úì Saved transaction baskets: 'transaction_baskets.csv'\n",
      "\n",
      "============================================================\n",
      "SUMMARY REPORT\n",
      "============================================================\n",
      "üìä DATA QUALITY IMPROVEMENT\n",
      "----------------------------------------\n",
      "Original records: 541,909\n",
      "Cleaned records: 392,692\n",
      "Data quality improvement: 27.5%\n",
      "\n",
      "‚úÖ CLEANING ACTIONS COMPLETED:\n",
      "  ‚Ä¢ Removed cancelled invoices\n",
      "  ‚Ä¢ Removed invalid quantities and prices\n",
      "  ‚Ä¢ Handled missing CustomerID and Description\n",
      "  ‚Ä¢ Standardized product descriptions\n",
      "  ‚Ä¢ Prepared transaction baskets for FP-Growth\n",
      "\n",
      "üéØ DATASET READY FOR ANALYSIS:\n",
      "  ‚Ä¢ 18,532 transactions\n",
      "  ‚Ä¢ 3,665 products\n",
      "  ‚Ä¢ 4,338 customers\n",
      "  ‚Ä¢ Time period: 2010-12-01 to 2011-12-09\n",
      "\n",
      "üìà NEXT STEPS:\n",
      "  ‚Ä¢ Proceed to Phase 3: FP-Growth Algorithm\n",
      "  ‚Ä¢ Generate association rules\n",
      "  ‚Ä¢ Implement temporal validation\n",
      "\n",
      "============================================================\n",
      "üéâ PHASES 1 & 2 COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Generated files:\n",
      "  ‚Ä¢ OnlineRetail.xlsx (original dataset)\n",
      "  ‚Ä¢ online_retail_cleaned.csv (cleaned data)\n",
      "  ‚Ä¢ transaction_baskets.csv (transaction data for FP-Growth)\n",
      "  ‚Ä¢ visualizations/ (EDA plots)\n",
      "\n",
      "You can now proceed to Phase 3: FP-Growth Algorithm\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION\n",
    "def main():\n",
    "    print(\"=== MARKET BASKET ANALYSIS - LOCAL TEST ===\")\n",
    "    print(\"This script will download the dataset and test Phases 1 & 2\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Download dataset\n",
    "    file_path = find_dataset()\n",
    "    if not file_path:\n",
    "        file_path = download_dataset()\n",
    "        if not file_path:\n",
    "            print(\"‚ùå Could not obtain dataset. Exiting.\")\n",
    "            return\n",
    "    \n",
    "    # Step 2: Test loading\n",
    "    # df = test_data_loading(file_path)\n",
    "    df = load_dataset(file_path)\n",
    "    if df is None:\n",
    "        print(\"‚ùå Failed to load dataset. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Phase 1 - Exploration\n",
    "    phase1_results = phase1_exploration(df)\n",
    "    \n",
    "    # Step 4: Create visualizations\n",
    "    create_visualizations(df)\n",
    "    \n",
    "    # Step 5: Phase 2 - Cleaning\n",
    "    df_clean, basket_data = phase2_cleaning(df, phase1_results)\n",
    "    \n",
    "    # Step 6: Summary report\n",
    "    create_summary_report(phase1_results, df_clean)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ PHASES 1 & 2 COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  ‚Ä¢ OnlineRetail.csv (original dataset)\")\n",
    "    print(\"  ‚Ä¢ online_retail_cleaned.csv (cleaned data)\")\n",
    "    print(\"  ‚Ä¢ transaction_baskets.csv (transaction data for FP-Growth)\")\n",
    "    print(\"  ‚Ä¢ visualizations/ (EDA plots)\")\n",
    "    \n",
    "    print(f\"\\nYou can now proceed to Phase 3: FP-Growth Algorithm\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20d238-8e80-4235-bcbf-3192b48bd051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:csgyc-6513-fall]",
   "language": "python",
   "name": "conda-env-csgyc-6513-fall-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
